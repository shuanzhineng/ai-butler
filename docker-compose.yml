version: '3'

services:
    aibutler_mysql:
      logging:
        driver: "json-file"
        options:
          max-size: "1g"
      image: mysql:8.0
      container_name: ai_butler_mysql
      volumes:
        - ./mysql/data:/var/lib/mysql
        - ./mysql/my.cnf:/etc/my.cnf
        - ./mysql/init:/docker-entrypoint-initdb.d
      ports:
        - "3406:3306"
      env_file:
        - .env
      command: [
        '--default-authentication-plugin=mysql_native_password',
        '--character-set-server=utf8mb4', #设置数据库表的数据集
        '--collation-server=utf8mb4_unicode_ci', #设置数据库表的数据集
      ]

    aibutler_minio:
      logging:
        driver: "json-file"
        options:
          max-size: "1g"
      image: minio/minio
      container_name: ai_butler_minio
      volumes:
        - ./minio/data:/data
      ports:
        - "9800:9000"
        - "9801:9001"
      env_file:
        - .env
      command: server /data --console-address ":9001"

    aibutler_redis:
      logging:
        driver: "json-file"
        options:
          max-size: "1g"
      restart: always
      image: redis:6.0
      container_name: ai_butler_redis
      command: redis-server /etc/redis/redis.conf
      ports:
        - "6389:6379"
      volumes:
        - ./redis/redis.conf:/etc/redis/redis.conf
        - ./redis/data:/data

    aibutler_backend:
        build:
            context: "./aibutler-backend"
            dockerfile: Dockerfile
        image: ai_butler_fastapi
        container_name: ai_butler_fastapi
        restart: always
        volumes:
            - ./aibutler-backend:/srv/app
        working_dir: /srv/app
        depends_on:
          - ai_butler_mysql
          - ai_butler_redis
          - ai_butler_minio
        ports:
            - "8488:8000"
        command:
            - /bin/sh
            - -c
            - |
                poetry run aerich upgrade
                poetry run uvicorn main:app --host=0.0.0.0 --port=8000  --workers=2

    aibutler_paddle_image_classify_gpu_worker:
        build:
            context: "./paddle-image-classify"
            dockerfile: Dockerfile.gpu
        image: ai_butler_paddle_image_classify_gpu
        container_name: ai_butler_paddle_image_classify_gpu_worker
        restart: always
        volumes:
            - ./paddle-image-classify:/srv/app
        env_file:
          - ./paddle-image-classify/.envs
        environment:
          - "NVIDIA_VISIBLE_DEVICES=all"
        depends_on:
          - aibutler_backend
        shm_size: "8g"
        deploy:
          resources:
            reservations:
              devices:
                - driver: "nvidia"
                  count: "all"
                  capabilities: [ "gpu" ]
        working_dir: /srv/app
        # 使用solo模式, 如果使用默认的进程模式会导致daemonic processes are not allowed to have children
        command: celery -A celery_app.app worker -P solo -l INFO -Q paddle_image_classify_train

    aibutler_pytorch_object_detection_gpu_worker:
        build:
            context: "./pytorch-object-detection"
            dockerfile: Dockerfile.gpu
        image: ai_butler_pytorch_object_detection_gpu
        container_name: ai_butler_pytorch_object_detection_gpu_worker
        restart: always
        volumes:
            - ./pytorch-object-detection:/srv/app
        env_file:
          - ./pytorch-object-detection/.envs
        environment:
          - "NVIDIA_VISIBLE_DEVICES=all"
        shm_size: "8g"
        depends_on:
          - aibutler_backend
        deploy:
          resources:
            reservations:
              devices:
                - driver: "nvidia"
                  count: "all"
                  capabilities: [ "gpu" ]
        working_dir: /srv/app
        # 使用solo模式, 如果使用默认的进程模式会导致daemonic processes are not allowed to have children
        command: celery -A celery_app.app worker -P solo -l INFO -Q pytorch_object_detection_train

    aibutler_deploy_predict_worker:
        build:
            context: "./deploy-predict"
            dockerfile: Dockerfile
        image: ai_butler_deploy_predict
        container_name: ai_butler_deploy_predict_worker
        restart: always
        volumes:
            - ./deploy-predict:/srv/app
        working_dir: /srv/app
        depends_on:
          - aibutler_backend
        env_file:
            - ./deploy-predict/.envs
        command: celery -A celery_app.app worker -c 1 -l INFO -Q deploy_predict

    aibutler_nginx:
      logging:
        driver: "json-file"
        options:
          max-size: "1g"
      restart: always
      image: nginx
      container_name: aibutler_nginx
      command: nginx -c /etc/nginx/nginx.conf
      depends_on:
        - aibutler_backend
      ports:
        - "8188:80"
      volumes:
        - ./aibutler-frontend/apps/frontend/:/etc/nginx/www-data/
        - ./aibutler-frontend/apps/frontend/nginx/:/etc/nginx/conf.d/
